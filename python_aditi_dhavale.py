# -*- coding: utf-8 -*-
"""Python_Aditi_Dhavale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8aZV1H0GwEYRJML16qWyT7GVEe_gpI6
"""

import numpy as np
import pandas as pd
import sklearn

from sklearn.datasets import load_boston
df = load_boston()

type(df)

df.keys()

print(df.feature_names)

print(df.target)

print(df.data)

print(df.DESCR) #info about dataset

boston = pd.DataFrame(df.data, columns=df.feature_names)
boston.head()

boston['MEDV'] = df.target 
boston.head()

boston.isnull().sum()

#The maximum and minimum values by columns
describe = boston.describe().loc[['min','max']]
describe

boston.describe()

boston.columns

# Histograms
df2 = boston.loc[:,boston.columns]

fig = plt.figure(figsize=(10, 10))
plt.suptitle('Histograms of Numerical Columns', fontsize=20)
for i in range(df2.shape[1]):
    plt.subplot(6, 3, i + 1)
    f = plt.gca()
    f.set_title(df2.columns.values[i])

    vals = np.size(df2.iloc[:, i].unique())
    if vals >= 100:
        vals = 100
    
    plt.hist(df2.iloc[:, i], bins=vals, color='#3F5D7D')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

from sklearn.model_selection import train_test_split
X = boston.drop('MEDV', axis=1)
Y = boston['MEDV']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 5)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

506*0.85

from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error

lin_model = LinearRegression()
lin_model.fit(X_train, Y_train)

y_train_predict = lin_model.predict(X_train)
rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict))) 
print("The model performance for trained data set")
print('RMSE is {}'.format(rmse))
print("\n")

y_test_predict = lin_model.predict(X_test)
rmse_test = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))
print("The model performance for test data set")
print('RMSE is {}'.format(rmse_test))
print("\n")

import matplotlib.pyplot as plt
plt.figure(figsize=(5,5))
plt.scatter(Y_test, y_test_predict)
plt.plot([min(y_test_predict), max(y_test_predict)], [min(y_test_predict), max(y_test_predict)])
plt.xlabel('Actual')
plt.ylabel('Predicted')

plt.figure(figsize=(5,5))
plt.scatter(Y_test, y_test_predict, c='crimson')
#plt.yscale('log')
#plt.xscale('log')

p1 = max(max(y_test_predict), max(Y_test))
p2 = min(min(y_test_predict), min(Y_test))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.axis('equal')
plt.show()

# Correlation with independent Variable
# To reduce RMSE value further, Correlation with independent Variable is calculated and feature scaling is applied
boston.columns
df2.corrwith(boston['MEDV']).plot.bar(
        figsize = (10, 10), title = "Correlation with MEDV", fontsize = 15,
        rot = 45, grid = True)

#Feature scaling
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
X_train = pd.DataFrame(sc_x.fit_transform(X_train), columns=X.columns.values)
X_test = pd.DataFrame(sc_x.transform(X_test), columns=X.columns.values)

lin_model_after_scaling = LinearRegression()
lin_model_after_scaling.fit(X_train, Y_train)

y_train_predict_after_scaling = lin_model_after_scaling.predict(X_train)
rmse_after_scaling = (np.sqrt(mean_squared_error(Y_train, y_train_predict_after_scaling))) 
print("The model performance for trained data set after scaling")
print('RMSE is {}'.format(rmse_after_scaling))
print("\n")

y_test_predict_after_scaling = lin_model_after_scaling.predict(X_test)
rmse_test_after_scaling = (np.sqrt(mean_squared_error(Y_test, y_test_predict_after_scaling)))
print("The model performance for test data set after scaling")
print('RMSE is {}'.format(rmse_test_after_scaling))
print("\n")

X1=X.drop(['CHAS'], axis=1)
X1.head()

X1=X1.drop(['DIS'], axis=1)
X1.head()

X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size = 0.15, random_state = 5)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

#training model by dropping two less correlated features CHAS and DIS, to lower RMSE values
lin_model_after_dropping_2lesscorrelatedfeatures = LinearRegression()
lin_model_after_dropping_2lesscorrelatedfeatures.fit(X_train, Y_train)

y_train_predict_after_dropping_2lesscorrelatedfeatures = lin_model_after_dropping_2lesscorrelatedfeatures.predict(X_train)
rmse_after_dropping_2lesscorrelatedfeatures = (np.sqrt(mean_squared_error(Y_train, y_train_predict_after_dropping_2lesscorrelatedfeatures))) 
print("The model performance for trained data set after dropping_2lesscorrelatedfeatures")
print('RMSE is {}'.format(rmse_after_dropping_2lesscorrelatedfeatures))
print("\n")

y_test_predict_after_dropping_2lesscorrelatedfeatures = lin_model_after_dropping_2lesscorrelatedfeatures.predict(X_test)
rmse_test_after_dropping_2lesscorrelatedfeatures = (np.sqrt(mean_squared_error(Y_test, y_test_predict_after_dropping_2lesscorrelatedfeatures)))
print("The model performance for test data set after dropping_2lesscorrelatedfeatures")
print('RMSE is {}'.format(rmse_test_after_dropping_2lesscorrelatedfeatures))
print("\n")

print("No improvement found after scaling features as well as after dropping two less correlated features in case of bostan data using linear regression.")